{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2cf18d1a-bd97-4da4-830c-4de488ad75af",
   "metadata": {},
   "source": [
    "### Dataset Exploration\n",
    "\n",
    "In this section, we will analyze relevant datasets to identify the most suitable ones for the **CallConnect** app. After reviewing available options, we have shortlisted the following datasets for detailed analysis:\n",
    "\n",
    "---\n",
    "\n",
    "### 1. Question-Answer Dataset  \n",
    "This dataset consists of question-answer pairs extracted from Wikipedia articles. It is ideal for training systems to generate accurate and concise answers to user queries, making it a valuable resource for building knowledge-driven chatbots.  \n",
    "\n",
    "- **Link:** [Question-Answer Dataset](https://www.cs.cmu.edu/~ark/QA-data/?ref=hackernoon.com)\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Ubuntu Dialogue Corpus  \n",
    "This dataset comprises over **26 million conversational turns** from two-person dialogues, derived from real-world conversations. It is well-suited for modeling multi-turn dialogues and enhancing contextual understanding in chatbot systems.  \n",
    "**Code to Download the Dataset:**\n",
    "\n",
    "```python\n",
    "import kagglehub\n",
    "\n",
    "# Download the latest version\n",
    "path = kagglehub.dataset_download(\"rtatman/ubuntu-dialogue-corpus\")\n",
    "\n",
    "print(\"Path to dataset files:\", path)\n",
    "```\n",
    "\n",
    "- **Link:** [Ubuntu Dialogue Corpus on Kaggle](https://www.kaggle.com/datasets/rtatman/ubuntu-dialogue-corpus)\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Customer Support on Twitter  \n",
    "This dataset includes over **3 million tweets and replies** exchanged between prominent brands and their customers on Twitter. It is particularly valuable for training chatbots to handle customer service interactions and provide prompt, accurate responses.  \n",
    "**Code to Download the Dataset:**\n",
    "\n",
    "```python\n",
    "import kagglehub\n",
    "\n",
    "# Download the latest version\n",
    "path = kagglehub.dataset_download(\"thoughtvector/customer-support-on-twitter\")\n",
    "\n",
    "print(\"Path to dataset files:\", path)\n",
    "```\n",
    "\n",
    "- **Link:** [Customer Support on Twitter](https://www.kaggle.com/datasets/thoughtvector/customer-support-on-twitter)\n",
    "\n",
    "---\n",
    "\n",
    "### 4. Switchboard Dialog Dataset  \n",
    "The **Switchboard Dialog Dataset** is a collection of over **2,400 human-to-human telephone conversations**. It focuses on task-oriented dialogues, providing valuable insights into goal-driven interactions. This dataset is especially useful for training chatbots designed to assist users with specific tasks or requests.  \n",
    "\n",
    "- **Documentation:** [Switchboard Dialog Dataset Docs](https://convokit.cornell.edu/documentation/switchboard.html)  \n",
    "- **GitHub Repository:** [Switchboard Dialog Dataset](https://github.com/cgpotts/swda)\n",
    "\n",
    "---\n",
    "\n",
    "### Additional Resources\n",
    "For further exploration of chatbot datasets:  \n",
    "- **Hackernoon Article:** [Top 15 Chatbot Datasets for NLP Projects](https://hackernoon.com/top-15-chatbot-datasets-for-nlp-projects-8k2f3zqc)  \n",
    "- **Dev Article:** [10 Useful Chatbot Datasets for NLP Projects](https://dev.to/devashishmamgain/10-useful-chatbot-datasets-for-nlp-projects-1fj2)  \n",
    "\n",
    "---\n",
    "\n",
    "### Selected Dataset for the **CallConnect** App  \n",
    "After careful consideration, we have decided to use the **Switchboard Dialog Dataset** as the primary dataset for our project. Its focus on real-world, task-oriented telephone conversations aligns well with the goals of the **CallConnect** app. Additional datasets, such as the **Ubuntu Dialogue Corpus**, **Customer Support on Twitter**, and the **Question-Answer Dataset**, will be leveraged to complement specific features of the app and enhance its overall performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39a6b769-3441-4830-af8a-e56e3e06cf85",
   "metadata": {},
   "source": [
    "### Reading the Switchboard Dialog Act Corpus (SwDA)\n",
    "\n",
    "We are using the **Switchboard Dialog Act Corpus (SwDA)** for our **Call Connect App** project. This dataset consists of **1,155 five-minute telephone conversations** between two participants, annotated with speech act tags. These conversations involve callers questioning receivers on various topics like child care, recycling, and news media. Here’s an overview of the structure, metadata, and how we plan to leverage this data for our app.\n",
    "\n",
    "---\n",
    "\n",
    "#### Dataset Highlights:\n",
    "- **Conversations:** 1,155 telephone conversations\n",
    "- **Speakers:** 440 participants\n",
    "- **Utterances:** 221,616 (after processing, we get 122,646 utterances)\n",
    "- **Topics:** The conversations revolve around prompts like \"child care\" or \"news media,\" which can be useful in analyzing various conversational aspects.\n",
    "\n",
    "#### Processed Dataset:\n",
    "To make this dataset ready for experimentation and modeling, we’re working with a processed version of it. The key modifications include:\n",
    "- **Disfluencies Removed:** Using regex to clean interruptions and non-verbal sounds that aren’t useful for our analysis.\n",
    "- **Backchannels Removed:** Utterances with fewer than 5 tokens (such as “uh” or “mm-hmm”) are excluded.\n",
    "- **Merged Turns:** Successive turns by the same speaker, interrupted by backchannels, are merged to make the conversation flow clearer.\n",
    "\n",
    "---\n",
    "\n",
    "#### Dataset Structure\n",
    "\n",
    "##### 1. **Speaker-Level Metadata**\n",
    "Each speaker has metadata that helps us understand the conversation dynamics:\n",
    "- **ID:** Unique identifier for each speaker.\n",
    "- **Sex:** The gender of the speaker (`MALE` or `FEMALE`).\n",
    "- **Education:** The speaker’s education level (e.g., 0: Less than high school, 1: Less than college, 2: College, 3: More than college, 9: Unknown).\n",
    "- **Birth Year:** The year the speaker was born.\n",
    "- **Dialect Area:** Regional dialect classification (e.g., `SOUTHERN`, `NYC`, `WESTERN`). This will help us tailor our app’s recognition systems.\n",
    "\n",
    "##### 2. **Utterance-Level Metadata**\n",
    "Each utterance corresponds to a turn in the conversation. We can use this metadata to structure our app’s responses and interactions:\n",
    "- **ID:** Formatted as `_conversation_id_`-`_utterance_position_` (e.g., `4325-0`).\n",
    "- **Speaker:** The person speaking.\n",
    "- **Text:** The content of the utterance.\n",
    "- **Tag:** Speech act tags (DAMSL) that classify the type of speech act (e.g., question, request).\n",
    "- **Alpha Text:** A version with only alphabetic tokens (for processed data).\n",
    "- **Reply To:** ID of the utterance this replies to (important for understanding conversation flow).\n",
    "- **Next ID:** ID of the next utterance replying to this one.\n",
    "\n",
    "##### 3. **Conversation-Level Metadata**\n",
    "This information helps us track the entire conversation:\n",
    "- **Filename:** The original file name in the SwDA dataset.\n",
    "- **Topic Description:** A brief description of the conversation’s topic.\n",
    "- **Prompt:** A detailed description of the conversation prompt, guiding the flow of conversation.\n",
    "- **Length:** The duration of the conversation in minutes.\n",
    "- **From Caller / To Caller:** The identifiers for the participants (A and B).\n",
    "\n",
    "---\n",
    "\n",
    "### Combining CSV Files for Model Training\n",
    "The SwDA dataset is split across multiple CSV files (one for each type of data: conversations, speakers, utterances). To train our models, we’ll need to consolidate these files into a single format that we can use effectively.\n",
    "\n",
    "#### Steps to Process:\n",
    "1. **Understand the Files:** We’ll begin by reviewing each file's content (such as the utterances, metadata, and speaker information).\n",
    "2. **Merge the Data:** We’ll combine the files based on common keys, like `conversation_id`, `speaker_id`, and `utterance_id`, so that all data about a conversation is in one place.\n",
    "3. **Clean the Data:** We’ll remove any unnecessary or noisy data (e.g., disfluencies, backchannels) to ensure our models only get the most relevant information.\n",
    "\n",
    "---\n",
    "\n",
    "### Documentation and References:\n",
    "- **SwDA Dataset Documentation:** [Switchboard Dialog Dataset Docs](https://convokit.cornell.edu/documentation/switchboard.html)  \n",
    "- **Original Paper:** [Dialogue Act Modeling for Automatic Tagging and Recognition of Conversational Speech (2000)](https://www.cs.cmu.edu/~ark/QA-data/?ref=hackernoon.com)  \n",
    "\n",
    "By merging this data into a unified format, we can build more accurate models for understanding conversations in our **Call Connect App**, making it smarter and more capable of handling natural dialogues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "03d81944-f30d-4d7f-b4ad-754dc192c8e0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample File is:\n",
      "\n",
      "                swda_filename ptb_basename  conversation_no  transcript_index  \\\n",
      "0    sw00utt/sw_0001_4325.utt     4/sw4325             4325                 0   \n",
      "1    sw00utt/sw_0001_4325.utt     4/sw4325             4325                 1   \n",
      "2    sw00utt/sw_0001_4325.utt     4/sw4325             4325                 2   \n",
      "3    sw00utt/sw_0001_4325.utt     4/sw4325             4325                 3   \n",
      "4    sw00utt/sw_0001_4325.utt     4/sw4325             4325                 4   \n",
      "..                        ...          ...              ...               ...   \n",
      "154  sw00utt/sw_0001_4325.utt     4/sw4325             4325               154   \n",
      "155  sw00utt/sw_0001_4325.utt     4/sw4325             4325               155   \n",
      "156  sw00utt/sw_0001_4325.utt     4/sw4325             4325               156   \n",
      "157  sw00utt/sw_0001_4325.utt     4/sw4325             4325               157   \n",
      "158  sw00utt/sw_0001_4325.utt     4/sw4325             4325               158   \n",
      "\n",
      "    act_tag caller  utterance_index  subutterance_index  \\\n",
      "0         o      A                1                   1   \n",
      "1        qw      A                1                   2   \n",
      "2      qy^d      B                2                   1   \n",
      "3         +      A                3                   1   \n",
      "4         +      B                4                   1   \n",
      "..      ...    ...              ...                 ...   \n",
      "154      sd      A               85                   3   \n",
      "155       %      B               86                   1   \n",
      "156       %      A               87                   1   \n",
      "157       %      A               87                   2   \n",
      "158       %      B               88                   1   \n",
      "\n",
      "                                                  text  \\\n",
      "0                                             Okay.  /   \n",
      "1                                             {D So, }   \n",
      "2                                       [ [ I guess, +   \n",
      "3    What kind of experience [ do you, + do you ] h...   \n",
      "4    I think, ] + {F uh, } I wonder ] if that worke...   \n",
      "..                                                 ...   \n",
      "154   {C and then } my youngest two have never been. /   \n",
      "155                                         {F Um. } /   \n",
      "156                           {D So } {D anyway, } - /   \n",
      "157                              {C but } we don't, -/   \n",
      "158                      {C So, } you've never had, -/   \n",
      "\n",
      "                                                   pos  \\\n",
      "0                                          Okay/UH ./.   \n",
      "1                                            So/UH ,/,   \n",
      "2                              [ I/PRP ] guess/VBP ,/,   \n",
      "3    [ What/WP kind/NN ] of/IN  [ experience/NN ] d...   \n",
      "4    [ I/PRP ] think/VBP ,/, uh/UH ,/,  [ I/PRP ] w...   \n",
      "..                                                 ...   \n",
      "154  and/CC then/RB  [ my/PRP$ youngest/JJS two/CD ...   \n",
      "155                                          Um/UH ./.   \n",
      "156                                So/RB anyway/UH ,/,   \n",
      "157               but/CC  [ we/PRP ] do/VBP n't/RB ,/,   \n",
      "158  So/RB ,/,  [ you/PRP ] 've/VBP never/RB had/VB...   \n",
      "\n",
      "                                                 trees ptb_treenumbers  \n",
      "0                   (INTJ (UH Okay) (. .) (-DFL- E_S))               1  \n",
      "1    (SBARQ (INTJ (UH So)) (, ,) (WHNP-1 (WHNP (WP ...               2  \n",
      "2    (S (EDITED (RM (-DFL- \\[)) (EDITED (RM (-DFL- ...               4  \n",
      "3    (SBARQ (INTJ (UH So)) (, ,) (WHNP-1 (WHNP (WP ...               2  \n",
      "4    (S (EDITED (RM (-DFL- \\[)) (EDITED (RM (-DFL- ...               4  \n",
      "..                                                 ...             ...  \n",
      "154  (S (CONJP (CC and) (RB then)) (NP-SBJ (PRP$ my...             226  \n",
      "155                   (INTJ (UH Um) (. .) (-DFL- E_S))             228  \n",
      "156  (S-UNF (INTJ (UH So)) (INTJ (UH anyway)) (, ,)...             230  \n",
      "157  (S (CC but) (NP-SBJ (PRP we)) (VP-UNF (VBP do)...             231  \n",
      "158  (S (RB So) (, ,) (NP-SBJ (PRP you)) (VP (VBP '...             233  \n",
      "\n",
      "[159 rows x 12 columns]\n"
     ]
    }
   ],
   "source": [
    "# Importing required libraries\n",
    "import pandas as pd\n",
    "\n",
    "# Reading one file from dataset to understand its structure\n",
    "file_sample = pd.read_csv(r\"data\\raw\\swda\\sw00utt\\sw_0001_4325.utt.csv\")\n",
    "\n",
    "print(\"Sample File is:\\n\")\n",
    "print(file_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d769c2be-b44b-42bf-90ce-4b8b3e6e7bc9",
   "metadata": {},
   "source": [
    "### Columns of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e56e811f-7c0e-44f7-831e-85e7b677639b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns are:\n",
      "\n",
      "Index(['swda_filename', 'ptb_basename', 'conversation_no', 'transcript_index',\n",
      "       'act_tag', 'caller', 'utterance_index', 'subutterance_index', 'text',\n",
      "       'pos', 'trees', 'ptb_treenumbers'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(\"Columns are:\\n\")\n",
    "\n",
    "print(file_sample.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49782d45-0c76-4f50-9b61-679510a72469",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
